# üìò EduSummarizer ‚Äî Building a Mini LLM from Scratch ( AI - Powered Notes Assistant )

EduSummarizer is a **custom-built Language Model developed entirely from scratch using PyTorch**, trained on educational text and deployed as a **full-stack LLM-style web application**.

Unlike API-based projects, this system focuses on understanding the **core internals of Large Language Models**‚Äîfrom tokenization and self-attention to training, inference, and deployment‚Äîwhile operating under **limited computational resources**.

This project demonstrates a complete **ML + NLP + DevOps workflow**, making it both an educational deep dive and a practical engineering system.

---

## üöÄ Key Features ( More reliable features will be added in the future )

- Custom **decoder-only Transformer (GPT-style)** architecture  
- Educational **text summarization and explanation**
- **SentencePiece (BPE)** tokenizer trained from scratch
- End-to-end **training and inference pipeline**
- **FastAPI backend** for model serving
- **Gradio / Streamlit frontend** for interactive usage
- Fully **Dockerized** application
- **Cloud deployment** on HuggingFace Spaces
- Designed to work in **low-resource environments**

---

## üéØ Motivation

Most existing summarization tools either:
- Use **rule-based heuristics** (low quality), or  
- Depend on **closed-source pretrained APIs** (low learning value)

EduSummarizer was built to:
- Understand LLMs **from first principles**
- Avoid black-box dependencies
- Explore how far a **small custom model** can go
- Demonstrate a complete **ML system lifecycle**
- Learn deployment and DevOps alongside modeling

---

## üß† Project Objectives

### Primary Objectives
- Build a **Mini LLM from scratch**
- Train it on educational datasets
- Deploy it as a usable **web-based application**

### Secondary Objectives
- Design and train a tokenizer
- Understand Transformer internals
- Practice ML deployment & MLOps workflows

### Out of Scope
- Billion-parameter models  
- Multimodal learning  
- Production-scale optimization  

---

## üèóÔ∏è System Architecture ( Flow of execution of this project )

1. Raw educational text ingestion  
2. SentencePiece tokenizer training (BPE)  
3. Decoder-only Transformer model  
4. Next-token prediction training  
5. Inference & text generation  
6. FastAPI-based serving layer  
7. Web UI (Gradio / Streamlit)  
8. Docker + Cloud deployment  

---

## üß© Model Architecture

The model follows a **GPT-style decoder-only Transformer**:

- Token Embeddings  
- Positional Embeddings  
- Multi-Head Self-Attention (causal masking)  
- Feed-Forward Neural Networks  
- Residual Connections  
- Layer Normalization (Pre-LN)  
- Linear Output Projection  

This architecture was chosen for **simplicity, interpretability, and efficiency**.

---

## üß† Transformer Internals (Conceptual Overview)

- **Self-Attention**: Captures contextual relationships across educational text  
- **Causal Masking**: Ensures autoregressive token generation  
- **Weight Sharing**: Reduces parameter count and improves generalization  
- **Pre-LayerNorm**: Improves training stability for deeper networks  

---

## üìö Dataset

### Data Sources
- Wikipedia (educational topics)
- NCERT summaries
- Lecture notes
- Open educational blogs

### Preprocessing Steps
- HTML and noise removal  
- Text normalization  
- Sentence cleanup  
- Single merged training corpus  

---

## üî§ Tokenizer

- Implemented using **SentencePiece (BPE)**
- Efficient handling of rare and unknown words
- Industry-standard tokenizer design used in modern LLMs

---

## üèãÔ∏è Training Pipeline

- Framework: **PyTorch**
- Objective: **Next-token prediction**
- Loss Function: **Cross Entropy Loss**
- Optimizer: **AdamW**
- Training Environment: Google Colab / Kaggle GPU
- Model Storage: HuggingFace Hub

---

## üìä Evaluation

The model is evaluated using standard summarization metrics:

- **ROUGE-1**
- **ROUGE-2**
- **ROUGE-L**

Baseline comparisons include:
- Lead-3 summarization
- TF-IDF-based summarizer

These benchmarks help validate model quality despite limited scale.

---

## üåê Backend API

The backend is implemented using **FastAPI**.

### Features
- Model loading and inference
- REST-based summarization endpoints
- Interactive API documentation via Swagger UI

---

## ‚òÅÔ∏è Deployment

- Frontend: **HuggingFace Spaces**
- Backend: **Render / Railway**
- Model Hosting: **HuggingFace Hub**

The application is fully cloud-hosted and hardware-independent.

---

## üîÑ Backup Plan (Plan B)

- Tool: **LlamaIndex**
- Purpose:
  - Fallback solution
  - Performance comparison
  - Reliability assurance

---

## üß∞ Tech Stack

### Programming
- Python
- PyTorch

### NLP
- SentencePiece

### Backend
- FastAPI
- Uvicorn

### Frontend
- Gradio / Streamlit

### DevOps
- Git
- GitHub
- Docker

### Cloud
- Google Colab
- HuggingFace Hub
- HuggingFace Spaces
- Render / Railway

---

## ‚ö†Ô∏è Limitations

- Small model size
- Limited long-context understanding
- Possible hallucinations
- No multimodal support

These limitations are acceptable for an **academic and learning-focused project**.

---

## üîÆ Future Enhancements

- Larger and deeper models
- Retrieval-Augmented Generation (RAG)
- Flashcard generation
- Multilingual support
- Voice-based interaction
- Domain-specific fine-tuning

---

## üöÄ How to Run EduSummarizer

### 1. Environment Setup
Clone the repository and create a virtual environment:

```
git clone "https://github.com/AbhinavvNair/EduSummarizer.git"
cd EduSummarizer
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Verify Model Files
Ensure your trained model and tokenizer are in the data/ directory:
```
data/edullm_model.pt (Weights)
data/tokenizer.model (SentencePiece model)
```

### 3. Launch the Web Application
Run the backend server using Uvicorn. This will serve both the AI API and the HTML frontend:
```
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### 4. Access the Interface
Open your browser and navigate to:
```
UI: http://localhost:8000
API Docs: http://localhost:8000/docs
```
